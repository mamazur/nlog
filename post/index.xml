<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Nina Loginova</title><link>https://gethugothemes.com/academia/site/post/</link><description>Recent content in Posts on Nina Loginova</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year}</copyright><atom:link href="https://gethugothemes.com/academia/site/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Computing and approximating multivariate chi-square probabilities</title><link>https://gethugothemes.com/academia/site/post/getting-started/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gethugothemes.com/academia/site/post/getting-started/</guid><description>&lt;p>Computing and approximating multivariate chi-square probabilities
The multivariate chi-squared distribution is an extension of the univariate chi-squared distribution to multiple variables. It is often used in multivariate statistics, particularly in hypothesis testing and multivariate analysis. Examples include comparing covariance matrices across groups, testing models in multivariate regression, or assessing the goodness-of-fit for multidimensional data.
If Z_1,Z_2,…,Z_ν are independent and identically distributed Gaussian random vectors with correlation matrix R, then the sum of their squares X follows multivariate chi-squared distribution.
The cumulative distribution function of the maximum of M dependent random variables X_1,X_2,…,X_M is
F_M (x)≡F_M (x,ν,R)≔P(⋂&lt;em>(j=1)^M▒{ X_j≤x})
where X=(X_1,…,X_M )^T has components X_j≔∑&lt;/em>(i=1)^ν▒Z_ij^2  &amp;ldquo;for&amp;rdquo; j=1,…,M.&lt;/p>
&lt;p>The multivariate chi-squared distribution does not have a simple closed-form expression for its distribution function. Its distribution function involves integrating over all possible values in a multidimensional space. In general, this kind of multidimensional integration cannot be easily solved analytically.
The R package mvtnorm lacks direct support for multivariate chi-squared distribution calculations, while PROC FREQ in SAS is limited to univariate and bivariate analyses, without tools for multivariate chi-squared probabilities. As a result, practical calculations rely on numerical methods or Monte Carlo simulations, particularly for high-dimensional cases.
Under some specific assumptions, there is a possibility to use methods from matrix factorization and probability theory to make these calculations easier and more efficient. (Stange et. al, 2015).
This computational methods “for evaluating and approximating M-variate chi-square probabilities for (in principle) arbitrary dimension M and arbitrary degrees of freedom” work if “the correlation structure in the k-variate marginal distributions is given (or can be approximated well) by low-rank correlation matrices” (Stange et. al, 2015). It means, that if “the underlying correlation matrix fulfills certain structural properties”, then quantiles [of multivariate chi-squared distribution] can be computed numerically. (Kim et. al, 2023). The main idea for approximating multivariate chi-squared distribution is to use the low rank factorization of the correlation matrices to break the integration down to computing lower dimensional marginal distributions, which is possible with standard computer programs.&lt;/p>
&lt;p>We developed MATLAB programs to implement these methods and demonstrated that our approach was both accurate and efficient by comparing it to traditional methods. We used real genetic data to show its practical application.
In this genetic association studies, our goal was to test whether there was a significant association between genetic variations and a particular phenotype. It led to simultaneous categorical data analysis, meaning that many contingency tables had to be analysed simultaneously, where every single contingency table summarizes study data for one position (locus) on the human genome. Locus-specific chi-squared statistics typically exhibit strong dependencies within blocks of genetic loci, because of the biological mechanism of inheritance. That means, the correlation matrices are low-rank and the assumptions needed for our computational methods were met, also with respect to the familywise error rate (FWER) control.
When testing many hypotheses, the probability of at least one false positive increases (inflated Type I error rates). This problem is usually addressed with multiple testing correction methods, like the Bonferroni correction or the False Discovery Rate (FDR). However, these methods assume that each test is independent, which is often not true in real-world data.
The effective number of tests tries to account for the dependence or correlation among tests. Instead of assuming that all tests are independent, it recognizes that correlated tests provide less new information than completely independent tests. By calculating the effective number of tests, we can adjust for the fact that some tests are not entirely new but share information with others.
A linkage disequilibrium (LD) matrix is a representation of the pairwise correlation between genetic variants within a particular region of the genome. It is used to quantify how much the presence of one genetic variant is non-randomly associated with another variant in a population.
In (Stange et. al, 2015) we compute “an effective number of tests of order 3 based on LD information taken from the international HapMap project (&lt;a href="http://hapmap.ncbi.nlm.nih.gov/)">http://hapmap.ncbi.nlm.nih.gov/)&lt;/a>. For exemplary purposes, we restrict our attention to loci on chromosome 21 in the CHD population (Chinese in Metropolitan Denver). In total this chromosome comprises M = 18,143 loci. We divided the chromosome into B =363 blocks of size 50 each (notice the last of these blocks comprises only 43 loci). We consider ν = 2 degrees of freedom corresponding to (2 × 3) contingency tables (i.e. to diploid genotypes) and make the assumption that loci from different blocks lead to stochastically independent χ2 test statistics. Under this assumption the effective number of tests can be calculated for every block separately, and the total effective number of tests is the sum of the block-specific ones.” “For the entire chromosome we obtained an effective number of tests of order 3 [equal to] 13,676, meaning that effectively only approximately two-third of the M = 18,143 loci contribute to the calibration with respect to the FWER level [0,05].”&lt;/p>
&lt;p>Figure 1. Block-specific effective numbers of tests for the real data example […], where the block size
equals 50. The effective number of tests varies between the blocks. The stronger the dependence among the test statistics
within a block, the smaller is the effective number of tests. The horizontal line displays the average effective number of
tests among all considered blocks. (Stange et al, 2015)&lt;/p>
&lt;p>Using numerical experiments we showed that our computational methods are faster and more efficient than traditional Monte Carlo simulations, allowing for precise calculations in less time. The efficiency of these methods can be increased with better techniques for matrix factorization, which would improve the accuracy of calculating the multivariate chi-square probabilities.&lt;/p>
&lt;p>&lt;em>Eunseop Kim, Steven N. MacEachern &amp;amp; Mario Peruggia (2023) Empirical likelihood for the analysis of experimental designs, Journal of Nonparametric Statistics, 35:4, 709-732, DOI: 10.1080/10485252.2023.2206919&lt;/em>&lt;/p>
&lt;p>&lt;em>Stange, J. &amp;amp; Loginova, N. &amp;amp; Dickhaus, Thorsten. (2015). Computing and approximating multivariate chi-square probabilities. Journal of Statistical Computation and Simulation. 86. 1-15. 10.1080/00949655.2015.1058798.&lt;/em>&lt;/p></description></item><item><title>Survival Analysis and Kaplan-Meier Estimate in Clinical Studies</title><link>https://gethugothemes.com/academia/site/post/jupyter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gethugothemes.com/academia/site/post/jupyter/</guid><description>&lt;p>Survival analysis is a key statistical method in clinical research, used to analyze the time until an event of interest, such as death, relapse, or recovery. Unlike traditional methods, it can handle censored data, where the event has not occurred for some subjects by the study&amp;rsquo;s end.
A central tool in survival analysis is the Kaplan-Meier estimate, a non-parametric method for estimating the survival function. The survival function S(t) represents the probability that a subject will survive beyond time t :&lt;/p>
&lt;p>S(t)=∏_(t_i≤t)▒(1-d_i/n_i )&lt;/p>
&lt;p>where t_i denotes the time of the i-th event, d_i is the number of events at t_i , and n_i is the number of individuals at risk just before t_i .
The Kaplan-Meier curve is constructed by recalculating the survival probability at each event time, accurately reflecting survival over time, for example for analyzing survival times of COVID-19 patients (Liu et. al, 2021).
In clinical studies, the Kaplan-Meier estimate is often used to compare the efficacy of treatments by plotting the survival curves for different patient groups. To test the significance of the difference between these survival curves, the log-rank test is commonly used. The log-rank statistic is calculated as:
χ^2=[∑_i▒(O_i-E_i ) ]^2/(∑_i▒V_i )
where O_i is the observed number of events in the i-th group, E_i is the expected number of events under the null hypothesis, V_i is the variance of the number of events.
The null hypothesis of the log-rank test is that there is no difference in the survival experience between the groups being compared. Specifically, it posits that the survival curves of the different groups are the same, meaning that the probability of the event (such as death or relapse) occurring at any given time point is the same across all groups.
In other words, under the null hypothesis, any observed differences in the survival curves are due to random variation rather than a true difference between the groups. If the log-rank test yields a p-value below a pre-specified significance level (e.g., 0.05), the null hypothesis is rejected, suggesting that there is a statistically significant difference in survival between the groups.&lt;/p>
&lt;p>Collett, D. (2015). Modelling survival data in medical research (3rd ed.). CRC Press. &lt;a href="https://doi.org/10.1201/b18348">https://doi.org/10.1201/b18348&lt;/a>
Liu X, Ahmad Z, Gemeay AM, Abdulrahman AT, Hafez EH, Khalil N (2021). Modeling the survival times of the COVID-19 patients with a new statistical model: A case study from China. PLoS ONE 16(7): e0254999. &lt;a href="https://doi.org/10.1371/journal.pone.0254999">https://doi.org/10.1371/journal.pone.0254999&lt;/a>&lt;/p></description></item></channel></rss>